{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f62d240",
   "metadata": {},
   "source": [
    "# Chapter 1: Getting Started\n",
    "\n",
    "Before taking a deep dive into machine learning and AI, let's get a handle on some of the basics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f04bf",
   "metadata": {},
   "source": [
    "## Core\n",
    "\n",
    "- Explanation\n",
    "\n",
    "#### Equations\n",
    "- Dot Product\n",
    "- Euclidean Distance\n",
    "- Bayes' theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8befcb4",
   "metadata": {},
   "source": [
    "### Dot Product\n",
    "A dot product, also called a scalar product, is something that we use to get a _scalar_ value after multiplying two vectors. It's used in a variety of Machine Learning and Data Analysis operations, so let's take a look at how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a048e",
   "metadata": {},
   "source": [
    "$\\mathbf{a}^\\top \\mathbf{b} = a \\cdot b = \\sum_{i=1}^{n}a_{i}b_{i} = a_{1}b_{1}+a_{2}b_{2} \\cdots + a_{n}b_{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d1098a",
   "metadata": {},
   "source": [
    "Just as a quick reminder, the transpose operation is essentially denoting a dot product. This will be especially relevent in Recommender System Cases, which we will get to later.\n",
    "\n",
    "Essentially, for n times (the amount of dimensions), multiply each vector term at the corresponding _dimension_, and add them all together. We can boil down our equation to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4a3b2",
   "metadata": {},
   "source": [
    "$ \\sum_{i=1}^{n}a_{i}b_{i} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949421d",
   "metadata": {},
   "source": [
    "I think the next step for computer scientists interested learning in AI is to try to code it. It's one thing to write down an equation, but it's another thing to make a functioning version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011ef5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(arry1,arry2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f883a30",
   "metadata": {},
   "source": [
    "For this first, _really_ simple implementation, we will make a function that takes two \"arrays\" (will be python lists to start) and multiple each of their terms at their index. We will assume that all cases will have equal length integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4675b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(arry1,arry2):\n",
    "    \n",
    "    assert isinstance(arry1, list), \"For this implementation, Array 1 must be a list\"\n",
    "    assert isinstance(arry2, list), \"For this implementation, Array 2 must be a list\"\n",
    "\n",
    "    final_scalar = 0\n",
    "    \n",
    "    array_list = zip(arry1,arry2)\n",
    "    for nums in (array_list):\n",
    "        final_scalar += nums[0]*nums[1]\n",
    "        \n",
    "    return final_scalar\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "256f9c1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product([1,2,3],[4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae097f15",
   "metadata": {},
   "source": [
    "### Great!\n",
    "\n",
    "That was your first implementation of a raw data science algorithm. Great job! Let's check out another!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c737b88",
   "metadata": {},
   "source": [
    "## Evaluating Models\n",
    "\n",
    "- Explanation\n",
    "\n",
    "#### Metrics\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c58909",
   "metadata": {},
   "source": [
    "## Normalizing Inputs\n",
    "\n",
    "- Explanation\n",
    "\n",
    "#### Algorithms\n",
    "- Softmax\n",
    "- Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5281876",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Next up, we'll talk about the Softmax Function. As [3blue1brown] calls it, it's one of the several \"squishification\" functions. Often Confused with the Sigmoid function, another \"squishification\" function, The softmax function (also called the normalized exponential function) is used to **normalize** the output of a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b522c977",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "- Explanation\n",
    "\n",
    "#### Algorithms ([Medium](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23) Article)\n",
    "- Wasserstein loss function\n",
    "- Least squares\n",
    "- Mean Square Error/Quadratic Loss/L2 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c4214",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "- Relu activation\n",
    "- Bayes' theorem\n",
    "- loss functions\n",
    "    - Gini Index\n",
    "    - L1\n",
    "    - L2\n",
    "    - dot-product loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5be5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
